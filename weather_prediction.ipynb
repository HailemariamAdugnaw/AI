{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weather-prediction-title"
   },
   "source": [
    "# Seattle Weather Prediction Model\n",
    "\n",
    "This notebook demonstrates how to build a machine learning model to predict weather conditions based on meteorological features.\n",
    "\n",
    "## Dataset Features:\n",
    "- **date**: Date of observation\n",
    "- **precipitation**: Amount of precipitation (mm)\n",
    "- **temp_max**: Maximum temperature (Â°C)\n",
    "- **temp_min**: Minimum temperature (Â°C)\n",
    "- **wind**: Wind speed (m/s)\n",
    "- **weather**: Weather type (drizzle, rain, sun, snow, fog) - **TARGET VARIABLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step-1-upload"
   },
   "source": [
    "## Step 1: Upload the CSV File to Google Colab\n",
    "\n",
    "Run the cell below to upload your `seattle-weather.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-file"
   },
   "source": [
    "# Upload the CSV file\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "# After running this cell, click 'Choose Files' and select 'seattle-weather.csv'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step-2-install"
   },
   "source": [
    "## Step 2: Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-libraries"
   },
   "source": [
    "# Install required libraries (if not already installed)\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-libraries"
   },
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step-3-load"
   },
   "source": [
    "## Step 3: Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-data"
   },
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('seattle-weather.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore-target"
   },
   "source": [
    "# Explore the target variable (weather)\n",
    "print(\"Weather Types Distribution:\")\n",
    "print(df['weather'].value_counts())\n",
    "\n",
    "# Visualize weather distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['weather'].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Weather Types', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Weather Type', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step-4-visualize"
   },
   "source": [
    "## Step 4: Data Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-features"
   },
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-distributions"
   },
   "source": [
    "# Distribution of features by weather type\n",
    "features = ['precipitation', 'temp_max', 'temp_min', 'wind']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    for weather_type in df['weather'].unique():\n",
    "        data = df[df['weather'] == weather_type][feature]\n",
    "        axes[idx].hist(data, alpha=0.5, label=weather_type, bins=30)\n",
    "    \n",
    "    axes[idx].set_xlabel(feature.replace('_', ' ').title(), fontsize=12)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[idx].set_title(f'Distribution of {feature.replace(\"_\", \" \").title()}', fontsize=14)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "box-plots"
   },
   "source": [
    "# Box plots for each feature by weather type\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    sns.boxplot(data=df, x='weather', y=feature, ax=axes[idx], palette='Set2')\n",
    "    axes[idx].set_title(f'{feature.replace(\"_\", \" \").title()} by Weather Type', fontsize=14)\n",
    "    axes[idx].set_xlabel('Weather Type', fontsize=12)\n",
    "    axes[idx].set_ylabel(feature.replace('_', ' ').title(), fontsize=12)\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step-5-preprocess"
   },
   "source": [
    "## Step 5: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature-engineering"
   },
   "source": [
    "# Create a copy of the dataframe for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Convert date to datetime and extract useful features\n",
    "df_processed['date'] = pd.to_datetime(df_processed['date'])\n",
    "df_processed['year'] = df_processed['date'].dt.year\n",
    "df_processed['month'] = df_processed['date'].dt.month\n",
    "df_processed['day'] = df_processed['date'].dt.day\n",
    "df_processed['day_of_week'] = df_processed['date'].dt.dayofweek\n",
    "df_processed['day_of_year'] = df_processed['date'].dt.dayofyear\n",
    "\n",
    "# Create additional features\n",
    "df_processed['temp_range'] = df_processed['temp_max'] - df_processed['temp_min']\n",
    "df_processed['is_freezing'] = (df_processed['temp_min'] <= 0).astype(int)\n",
    "\n",
    "# Drop the original date column\n",
    "df_processed = df_processed.drop('date', axis=1)\n",
    "\n",
    "print(\"Processed dataset shape:\", df_processed.shape)\n",
    "print(\"\\nNew features added!\")\n",
    "print(df_processed.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare-data"
   },
   "source": [
    "# Separate features and target\n",
    "X = df_processed.drop('weather', axis=1)\n",
    "y = df_processed['weather']\n",
    "\n",
    "print(\"Feature columns:\")\n",
    "print(X.columns.tolist())\n",
    "print(\"\\nFeatures shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"\\nWeather type encoding:\")\n",
    "for i, weather_type in enumerate(label_encoder.classes_):\n",
    "    print(f\"{weather_type}: {i}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split-data"
   },
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "print(f\"\\nTesting set distribution:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scale-features"
   },
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"\\nScaled training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing data shape: {X_test_scaled.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step-6-train"
   },
   "source": [
    "## Step 6: Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define-models"
   },
   "source": [
    "# Define multiple models to compare\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Support Vector Machine': SVC(kernel='rbf', random_state=42)\n",
    "}\n",
    "\n",
    "print(\"Models defined:\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"  - {model_name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-models"
   },
   "source": [
    "# Train and evaluate all models\n",
    "results = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[model_name] = accuracy\n",
    "    \n",
    "    print(f\"{model_name} Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"All models trained successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step-7-evaluate"
   },
   "source": [
    "## Step 7: Model Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare-models"
   },
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "model_names = list(results.keys())\n",
    "accuracies = list(results.values())\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))\n",
    "bars = plt.bar(model_names, accuracies, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height*100:.2f}%',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.title('Model Accuracy Comparison', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = max(results, key=results.get)\n",
    "best_accuracy = results[best_model_name]\n",
    "\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
    "print(f\"ðŸŽ¯ Best Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detailed-evaluation"
   },
   "source": [
    "# Detailed evaluation of the best model\n",
    "best_model = models[best_model_name]\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Detailed Evaluation: {best_model_name}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=label_encoder.classes_))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confusion-matrix"
   },
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted Weather', fontsize=12)\n",
    "plt.ylabel('Actual Weather', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step-8-feature-importance"
   },
   "source": [
    "## Step 8: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature-importance"
   },
   "source": [
    "# Feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
    "    plt.title(f'Feature Importance - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Importance Score', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    print(feature_importance.head())\n",
    "else:\n",
    "    print(f\"Feature importance not available for {best_model_name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step-9-predictions"
   },
   "source": [
    "## Step 9: Make Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sample-prediction"
   },
   "source": [
    "# Example: Make a prediction for custom weather data\n",
    "def predict_weather(precipitation, temp_max, temp_min, wind, month, day):\n",
    "    \"\"\"\n",
    "    Predict weather based on input parameters\n",
    "    \n",
    "    Parameters:\n",
    "    - precipitation: Amount of precipitation (mm)\n",
    "    - temp_max: Maximum temperature (Â°C)\n",
    "    - temp_min: Minimum temperature (Â°C)\n",
    "    - wind: Wind speed (m/s)\n",
    "    - month: Month (1-12)\n",
    "    - day: Day of month (1-31)\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    \n",
    "    # Create a sample date for the given month and day\n",
    "    year = 2023  # arbitrary year\n",
    "    date = datetime.datetime(year, month, day)\n",
    "    \n",
    "    # Calculate derived features\n",
    "    day_of_week = date.weekday()\n",
    "    day_of_year = date.timetuple().tm_yday\n",
    "    temp_range = temp_max - temp_min\n",
    "    is_freezing = 1 if temp_min <= 0 else 0\n",
    "    \n",
    "    # Create feature array in the same order as training data\n",
    "    features = np.array([[\n",
    "        precipitation, temp_max, temp_min, wind,\n",
    "        year, month, day, day_of_week, day_of_year,\n",
    "        temp_range, is_freezing\n",
    "    ]])\n",
    "    \n",
    "    # Scale features\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = best_model.predict(features_scaled)\n",
    "    predicted_weather = label_encoder.inverse_transform(prediction)[0]\n",
    "    \n",
    "    # Get prediction probabilities if available\n",
    "    if hasattr(best_model, 'predict_proba'):\n",
    "        probabilities = best_model.predict_proba(features_scaled)[0]\n",
    "        prob_dict = {label_encoder.classes_[i]: prob \n",
    "                    for i, prob in enumerate(probabilities)}\n",
    "        return predicted_weather, prob_dict\n",
    "    else:\n",
    "        return predicted_weather, None\n",
    "\n",
    "# Test the prediction function\n",
    "print(\"Example Predictions:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example 1: Rainy conditions\n",
    "print(\"\\n1. Rainy conditions:\")\n",
    "print(\"   Input: precipitation=15.0, temp_max=10.0, temp_min=5.0, wind=5.0, month=11, day=15\")\n",
    "weather, probs = predict_weather(15.0, 10.0, 5.0, 5.0, 11, 15)\n",
    "print(f\"   Predicted Weather: {weather}\")\n",
    "if probs:\n",
    "    print(\"   Probabilities:\")\n",
    "    for w, p in sorted(probs.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"     {w}: {p*100:.2f}%\")\n",
    "\n",
    "# Example 2: Sunny conditions\n",
    "print(\"\\n2. Sunny conditions:\")\n",
    "print(\"   Input: precipitation=0.0, temp_max=25.0, temp_min=15.0, wind=2.0, month=7, day=20\")\n",
    "weather, probs = predict_weather(0.0, 25.0, 15.0, 2.0, 7, 20)\n",
    "print(f\"   Predicted Weather: {weather}\")\n",
    "if probs:\n",
    "    print(\"   Probabilities:\")\n",
    "    for w, p in sorted(probs.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"     {w}: {p*100:.2f}%\")\n",
    "\n",
    "# Example 3: Snowy conditions\n",
    "print(\"\\n3. Snowy conditions:\")\n",
    "print(\"   Input: precipitation=10.0, temp_max=0.0, temp_min=-5.0, wind=3.0, month=1, day=10\")\n",
    "weather, probs = predict_weather(10.0, 0.0, -5.0, 3.0, 1, 10)\n",
    "print(f\"   Predicted Weather: {weather}\")\n",
    "if probs:\n",
    "    print(\"   Probabilities:\")\n",
    "    for w, p in sorted(probs.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"     {w}: {p*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step-10-save"
   },
   "source": [
    "## Step 10: Save the Trained Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-model"
   },
   "source": [
    "# Save the model and preprocessing objects\n",
    "import pickle\n",
    "\n",
    "# Save the best model\n",
    "with open('weather_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the label encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"Model and preprocessing objects saved successfully!\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Model file: weather_model.pkl\")\n",
    "print(f\"Scaler file: scaler.pkl\")\n",
    "print(f\"Label encoder file: label_encoder.pkl\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-model"
   },
   "source": [
    "# Download the saved models from Colab\n",
    "from google.colab import files\n",
    "\n",
    "files.download('weather_model.pkl')\n",
    "files.download('scaler.pkl')\n",
    "files.download('label_encoder.pkl')\n",
    "\n",
    "print(\"Models downloaded! You can now use them for predictions in other applications.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "1. âœ… Loaded and explored the Seattle weather dataset\n",
    "2. âœ… Visualized data distributions and correlations\n",
    "3. âœ… Engineered new features from the date column\n",
    "4. âœ… Trained multiple machine learning models\n",
    "5. âœ… Compared model performances\n",
    "6. âœ… Evaluated the best model with detailed metrics\n",
    "7. âœ… Analyzed feature importance\n",
    "8. âœ… Created a prediction function for new data\n",
    "9. âœ… Saved the trained model for future use\n",
    "\n",
    "### Next Steps:\n",
    "- Try different feature engineering techniques\n",
    "- Experiment with hyperparameter tuning\n",
    "- Use cross-validation for more robust evaluation\n",
    "- Try deep learning models (Neural Networks)\n",
    "- Deploy the model as a web application\n",
    "\n",
    "### Key Insights:\n",
    "- The model can predict weather types with good accuracy\n",
    "- Temperature and precipitation are typically important features\n",
    "- Different weather types have distinct patterns in the data\n",
    "\n",
    "**Thank you for using this notebook! Happy predicting! ðŸŒ¤ï¸**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Seattle Weather Prediction",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
